# 检测点云中的目标(ROS2 & Tao-PointPillars)

![](https://developer-blogs.nvidia.com/wp-content/uploads/2022/09/automotive-deepmap-drive-sim-stuttgart-radar-localization-1832800.jpg)

准确、快速的目标检测是机器人导航和避障中的一项重要任务。 自主代理需要清晰的周围环境地图才能导航到目的地，同时避免碰撞。 例如，在使用自主移动机器人 (AMR) 运输物体的仓库中，避免可能损坏机器人的危险机器已成为一个具有挑战性的问题。

这篇文章介绍了一个 ROS 2 节点，用于使用 [NVIDIA TAO Toolkit](https://developer.nvidia.com/tao-toolkit) 基于 [PointPillars](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet) 的预训练模型检测点云中的对象。 该节点将点云作为来自真实或模拟激光雷达扫描的输入，执行 TensorRT 优化推理以检测此输入数据中的对象，并将生成的 3D 边界框作为每个点云的 [Detection3DArray](http://docs.ros.org/en/lunar/api/vision_msgs/html/msg/Detection3DArray.html) 消息输出。

虽然存在多个 ROS 节点用于从图像中进行对象检测，但从激光雷达输入执行对象检测的优点包括：

* 激光雷达可以同时计算到许多检测到的物体的准确距离。 通过激光雷达直接提供的物体距离和方向信息，可以获得准确的环境 3D 地图。 为了在基于相机/图像的系统中获得相同的信息，需要一个单独的距离估计过程，这需要更多的计算能力。
* 与相机不同，激光雷达对不断变化的照明条件（包括阴影和强光）不敏感。

通过结合使用激光雷达和摄像头，可以使自治系统更加稳健。 这是因为摄像头可以执行激光雷达无法执行的任务，例如检测标志上的文字。

TAO-PointPillars 基于论文 [PointPillars: Fast Encoders for Object Detection from Point Clouds](https://arxiv.org/abs/1812.05784) 中介绍的工作，该论文描述了一种编码器，用于从垂直列（或柱子）中组织的点云中学习特征。 TAO-PointPillars 使用了编码特征以及论文中描述的下游检测网络。

对于我们的工作，[PointPillar 模型](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tao/models/pointpillarnet/files)在由 Zvision 的固态激光雷达收集的点云数据集上进行了训练。 PointPillar 模型检测三类对象：车辆、行人和骑自行车的人。 您可以按照 [TAO Toolkit 3D 对象检测](https://docs.nvidia.com/tao/tao-toolkit/text/point_cloud/index.html)步骤训练自己的检测模型，并将其与此节点一起使用。

有关运行节点的详细信息，请访问 GitHub 上的 [NVIDIA-AI-IOT/ros2_tao_pointpillars](https://github.com/NVIDIA-AI-IOT/ros2_tao_pointpillars)。 您还可以查看 [NVIDIA Isaac ROS](https://developer.nvidia.com/isaac-ros)，了解 NVIDIA 为各种感知任务提供的更多硬件加速 ROS 2 包。

![](https://developer-blogs.nvidia.com/wp-content/uploads/2022/09/image4-5.png)


## ROS 2 TAO-PointPillars 节点
本节提供有关在机器人应用程序中使用 ROS 2 TAO-PointPillars 节点的更多详细信息，包括输入/输出格式以及如何可视化结果。

**节点输入**：节点将点云作为 [PointCloud2](http://docs.ros.org/en/lunar/api/sensor_msgs/html/msg/PointCloud2.html) 消息格式的输入。 除其他信息外，点云必须包含每个点 (x, y, z, r) 的四个特征，其中 (x, y, z, r) 分别表示 X 坐标、Y 坐标、Z 坐标和反射率（强度）。

反射率表示在 3D 空间中某个点反射回来的激光束的分数。 请注意，训练数据和推理数据中反射率值的范围应相同。 可以从节点的启动文件中设置包括强度范围、类名、NMS IOU 阈值在内的参数。

您可以通过访问 GitHub 上的 [ZVISION-lidar/zvision_ugv_data](https://github.com/ZVISION-lidar/zvision_ugv_data) 找到用于测试节点的 ROS 2 包。

![](https://developer-blogs.nvidia.com/wp-content/uploads/2022/09/image2-6.png)

**节点输出**：节点以 [Detection3DArray](http://docs.ros.org/en/lunar/api/vision_msgs/html/msg/Detection3DArray.html) 消息格式输出点云中检测到的每个对象的 3D 边界框信息、对象类 ID 和分数。 每个 3D 边界框由 (x, y, z, dx, dy, dz, yaw) 表示，其中 (x, y, z, dx, dy, dz, yaw) 分别是对象中心的 X 坐标，Y 3D欧几里得空间中物体中心坐标、物体中心Z坐标、长度（X方向）、宽度（Y方向）、高度（Z方向）和方向。

模型在训练期间使用的坐标系和输入数据在推理期间使用的坐标系必须相同才能获得有意义的结果。 图 3 显示了 TAO-PointPillars 模型使用的坐标系。

![](https://developer-blogs.nvidia.com/wp-content/uploads/2022/09/image3-1-1.png)

由于目前无法在 RViz 上可视化 Detection3DArray 消息，您可以通过访问 GitHub 上的 [NVIDIA-AI-IOT/viz_3Dbbox_ros2_pointpillars](https://github.com/NVIDIA-AI-IOT/viz_3Dbbox_ros2_pointpillars) 找到一个简单的工具来可视化结果。

对于下面图所示的示例，在 Jetson AGX Orin 上，输入点云的频率约为 10 FPS，输出 Detection3DArray 消息的频率约为 10 FPS。

![](https://developer-blogs.nvidia.com/wp-content/uploads/2022/09/image5.gif)


## 总结
实时准确的对象检测对于自主代理安全导航其环境是必要的。 这篇文章展示了一个 ROS 2 节点，它可以使用预训练的 TAO-PointPillars 模型检测点云中的对象。 （请注意，该模型的 TensorRT 引擎目前仅支持批量大小为 1。）该模型直接对激光雷达输入执行推理，这与使用基于图像的方法相比具有优势。 为了对激光雷达数据进行推理，必须使用对来自同一激光雷达的数据进行训练的模型。 否则准确性将显着下降，除非实施统计标准化等方法。






















